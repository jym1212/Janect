# -*- coding: utf-8 -*-
"""Bert_QA_tokenizer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14vOyygm9sTiGHRqvtOIuflX5Zar85gGX
"""

!pip install pdfminer.six
!pip install transformers
!pip install datasets
!pip install pypdf

import torch
import pandas as pd
from pdfminer.high_level import extract_text
from transformers import pipeline, AutoTokenizer, AutoModelForQuestionAnswering, LongformerTokenizer, LongformerForQuestionAnswering
from datasets import Dataset
from pypdf import PdfReader
import chardet
import nltk
nltk.download('punkt')

# Function to detect encoding of text in a file
def detect_encoding(file_path):
    with open(file_path, 'rb') as f:
        result = chardet.detect(f.read())
    return result['encoding']

def pdf_to_qa_dataset(pdf_path, output_csv_path, max_length=512):
    """Converts a PDF (with potential Korean UTF-8 text) to a Q&A dataset."""

    encoding = detect_encoding(pdf_path)

    text = ""
    reader = PdfReader(pdf_path)
    for page in reader.pages:
        page_text = page.extract_text(encoding=encoding)
        text += page_text + "\n"

    # Use a Korean-specific tokenizer and model if needed
    model_name = "bert-large-uncased-whole-word-masking-finetuned-squad"  # Replace with a Korean-specific model if necessary
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForQuestionAnswering.from_pretrained(model_name)

    if "longformer" in model_name:
        tokenizer = LongformerTokenizer.from_pretrained(model_name)
        model = LongformerForQuestionAnswering.from_pretrained(model_name)

    # Customize special tokens (remove [SEP] since we're not using sentence pairs)
    tokenizer.add_special_tokens({'sep_token': '[unused1]'})


    qa_pipeline = pipeline("question-answering", model=model, tokenizer=tokenizer)
    qa_pairs = []

    for paragraph in text.split("\n\n"):
        for sentence in nltk.sent_tokenize(paragraph):
            chunk = sentence

            # Tokenize with truncation and padding (remove the [SEP] token)
            inputs = tokenizer(chunk, padding="max_length", truncation=True,
                               return_tensors="pt", max_length=max_length, add_special_tokens=False)

            # Generate questions and answers
            outputs = model(**inputs)
            start_index = torch.argmax(outputs.start_logits)
            end_index = torch.argmax(outputs.end_logits)
            question = tokenizer.decode(inputs["input_ids"][0][start_index:end_index+1])

            if question:  # Check if question is not empty
                answer = qa_pipeline(question=question, context=chunk)['answer']
                qa_pairs.append({"question": question, "answer": answer, "context": chunk})

    # Create and save the dataset
    qa_dataset = Dataset.from_list(qa_pairs)
    qa_dataset.to_csv(output_csv_path, index=False)
    return qa_dataset

# Define the paths
pdf_path = "/content/drive/MyDrive/dataset/CertificateDatasets/2024Certplan.pdf"
output_csv_path = "/content/drive/MyDrive/dataset/CertificateDatasets/2024Certplan_qa.csv"

# Generate Q&A dataset
qa_dataset = pdf_to_qa_dataset(pdf_path, output_csv_path)

# Load and convert to JSONL if the dataset was created
if qa_dataset:
    df = pd.read_csv(output_csv_path)
    jsonl_file_path = '2024Certplain_qa.jsonl'  # Updated file name
    df.to_json(jsonl_file_path, orient='records', lines=True)
else:
    print("Error: CSV file was not created.")

