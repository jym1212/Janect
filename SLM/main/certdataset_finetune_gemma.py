# -*- coding: utf-8 -*-
"""Certdataset-finetune-gemma.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YXav3d4ArxD1Ole1Jb_yDo1Hk43igoEC

## Prerequisites

Before delving into the fine-tuning process, ensure that you have the following prerequisites in place:

1. **GPU**: [gemma-2b](https://huggingface.co/google/gemma-2b) - can be finetuned on T4(free google colab) while [gemma-7b](https://huggingface.co/google/gemma-7b) requires an A100 GPU.
2. **Python Packages**: Ensure that you have the necessary Python packages installed. You can use the following commands to install them:

Let's begin by checking if your GPU is correctly detected:
"""

!nvidia-smi

"""## Step 2 - Model loading
We'll load the model using QLoRA quantization to reduce the usage of memory

"""

!pip3 install -q -U bitsandbytes==0.42.0
!pip3 install -q -U peft==0.8.2
!pip3 install -q -U trl==0.7.10
!pip3 install -q -U accelerate==0.27.1
!pip3 install -q -U datasets==2.17.0
!pip3 install -q -U transformers==4.38.0

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

"""Now we specify the model ID and then we load it with our previously defined quantization configuration.Now we specify the model ID and then we load it with our previously defined quantization configuration."""

# if you are using google colab

import os
from google.colab import userdata
os.environ["HF_TOKEN"] = userdata.get('HF_TOKEN')

from huggingface_hub import notebook_login
notebook_login()

# model_id = "google/gemma-7b-it"
# model_id = "google/gemma-7b"
# model_id = "google/gemma-2b-it"
model_id = "google/gemma-2b"

model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={"":0})
tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)

def get_completion(query: str, model, tokenizer) -> str:
  device = "cuda:0"

  prompt_template = """
  <start_of_turn>user
  Below is an instruction that describes a task. Write a response that appropriately completes the request.
  {query}
  <end_of_turn>\n<start_of_turn>model


  """
  prompt = prompt_template.format(query=query)

  encodeds = tokenizer(prompt, return_tensors="pt", add_special_tokens=True)

  model_inputs = encodeds.to(device)


  generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)
  # decoded = tokenizer.batch_decode(generated_ids)
  decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
  return (decoded)

result = get_completion(query="code the fibonacci series in python using reccursion", model=model, tokenizer=tokenizer)
print(result)

"""## Step 3 - Load dataset for finetuning

### Lets Load the Dataset

For this tutorial, we will fine-tune Mistral 7B Instruct for code generation.

We will be using this [dataset](https://huggingface.co/datasets/TokenBender/code_instructions_122k_alpaca_style) which is curated by [TokenBender (e/xperiments)](https://twitter.com/4evaBehindSOTA) and is an excellent data source for fine-tuning models for code generation. It follows the alpaca style of instructions, which is an excellent starting point for this task. The dataset structure should resemble the following:

```json
{
  "instruction": "Create a function to calculate the sum of a sequence of integers.",
  "input": "[1, 2, 3, 4, 5]",
  "output": "# Python code def sum_sequence(sequence): sum = 0 for num in sequence: sum += num return sum"
}
```
"""

from datasets import load_dataset
import pandas as pd
from pandas import json_normalize

dataset = load_dataset("TokenBender/code_instructions_122k_alpaca_style", split="train")
dataset

df = dataset.to_pandas()
df.head(10)

custom_data = {
  "features": ["output", "instruction", "text", "input"],
  "data": [
    {
      "output": "Details about the certification exam results and statistics.",
      "instruction": "Summarize the exam results for the technician category.",
      "text": "The technician exam had a total of 48267 applicants with a 65.3% pass rate.",
      "input": "Data from 2023 National Certification Statistics Report"
    },
    {
      "output": "Details about the certification exam results and statistics.",
      "instruction": "Provide the exam details for the industrial technician session 1.",
      "text": "The industrial technician exam had a total of 18603 applicants with a 56.9% pass rate.",
      "input": "Data from 2023 National Certification Statistics Report"
    },
    {
      "output": "Details about the certification exam results and statistics.",
      "instruction": "Explain the pass rates for the master craftsman category.",
      "text": "The master craftsman exam had a total of 2828 applicants with a 63.2% pass rate.",
      "input": "Data from 2023 National Certification Statistics Report"
    },
    {
      "output": "Details about the service sector exam results.",
      "instruction": "Summarize the pass rates and applicant numbers for the service sector exams.",
      "text": "The service sector exam had 2868 applicants in total with a pass rate of 57.5%.",
      "input": "Service sector data from 2023 report"
    },
    {
      "output": "Analysis of gender distribution in exam results.",
      "instruction": "Discuss the gender distribution in the technician certification exams.",
      "text": "In the technician category, of the 43838 male applicants, 22658 passed; of the 4438 female applicants, 1671 passed.",
      "input": "2023 Certification Examination Report"
    },
    {
      "output": "Statistical analysis of exam trends over five years.",
      "instruction": "Analyze the trend of pass rates over the last five years for the service sector.",
      "text": "Exam data from 2018 to 2023 shows fluctuating pass rates in the service sector, highlighting challenges and improvements in training programs.",
      "input": "Service sector examination trend data from 2018 to 2023"
    },
    {
      "output": "Detailed statistics on exam performance by region.",
      "instruction": "Provide insights on the regional performance differences in the technician exams.",
      "text": "Technician certification exams show varying success rates across regions, with urban areas generally performing better due to higher access to resources and training centers.",
      "input": "Regional data on technician certification exams from 2023"
    },
    {
      "output": "Longitudinal analysis of certification uptake.",
      "instruction": "Examine the growth in the number of applicants for industrial certifications over the past decade.",
      "text": "There has been a steady increase in the number of applicants for industrial certifications, reflecting the growing demand for skilled labor in manufacturing and engineering sectors.",
      "input": "Decadal data on certification applicants from 2013 to 2023"
    }
  ],
  "num_rows": 121959
}

# Handling nested data by converting relevant parts into separate DataFrames
related_terms_df = pd.DataFrame([custom_data['related_terms']])
data_df = pd.DataFrame(custom_data['data'])

# Drop the nested fields from the main dictionary to avoid conversion issues
del custom_data['related_terms']
del custom_data['data']

# Convert the rest of the custom data into a DataFrame
summary_df = pd.DataFrame([custom_data])  # Enclose in a list to handle top-level fields
# Concatenate all parts into a single DataFrame
extended_df = pd.concat([summary_df, related_terms_df, data_df], ignore_index=True)

# Display the resulting extended DataFrame
print(extended_df.head())

"""Instruction Fintuning - Prepare the dataset under the format of "prompt" so the model can better understand :
1. the function generate_prompt : take the instruction and output and generate a prompt
2. shuffle the dataset
3. tokenizer the dataset

### Formatting the Dataset

Now, let's format the dataset in the required [gemma instruction formate](https://huggingface.co/google/gemma-7b-it).

> Many tutorials and blogs skip over this part, but I feel this is a really important step.

```
<start_of_turn>user What is your favorite condiment? <end_of_turn>
<start_of_turn>model Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavor to whatever I'm cooking up in the kitchen!<end_of_turn>
```

You can use the following code to process your dataset and create a JSONL file in the correct format:
"""

def generate_prompt(data_point):
    """Gen. input text based on a prompt, task instruction, (context info.), and answer

    :param data_point: dict: Data point
    :return: dict: tokenzed prompt
    """
    prefix_text = 'Below is an instruction that describes a task. Write a response that ' \
               'appropriately completes the request.\n\n'
    # Samples with additional context into.
    if data_point['input']:
        text = f"""<start_of_turn>user {prefix_text} {data_point["instruction"]} here are the inputs {data_point["input"]} <end_of_turn>\n<start_of_turn>model{data_point["output"]} <end_of_turn>"""
    # Without
    else:
        text = f"""<start_of_turn>user {prefix_text} {data_point["instruction"]} <end_of_turn>\n<start_of_turn>model{data_point["output"]} <end_of_turn>"""
    return text

# add the "prompt" column in the dataset
text_column = [generate_prompt(data_point) for data_point in dataset]
dataset = dataset.add_column("prompt", text_column)

"""We'll need to tokenize our data so the model can understand.

"""

dataset = dataset.shuffle(seed=1234)  # Shuffle dataset here
dataset = dataset.map(lambda samples: tokenizer(samples["prompt"]), batched=True)

"""Split dataset into 90% for training and 10% for testing"""

dataset = dataset.train_test_split(test_size=0.2)
train_data = dataset["train"]
test_data = dataset["test"]

"""### After Formatting, We should get something like this

```json
{
"text":"<start_of_turn>user Create a function to calculate the sum of a sequence of integers. here are the inputs [1, 2, 3, 4, 5] <end_of_turn>
<start_of_turn>model # Python code def sum_sequence(sequence): sum = 0 for num in sequence: sum += num return sum <end_of_turn>",
"instruction":"Create a function to calculate the sum of a sequence of integers",
"input":"[1, 2, 3, 4, 5]",
"output":"# Python code def sum_sequence(sequence): sum = 0 for num in,
 sequence: sum += num return sum",
"prompt":"<start_of_turn>user Create a function to calculate the sum of a sequence of integers. here are the inputs [1, 2, 3, 4, 5] <end_of_turn>
<start_of_turn>model # Python code def sum_sequence(sequence): sum = 0 for num in sequence: sum += num return sum <end_of_turn>"

}
```

While using SFT (**[Supervised Fine-tuning Trainer](https://huggingface.co/docs/trl/main/en/sft_trainer)**) for fine-tuning, we will be only passing in the ‚Äútext‚Äù column of the dataset for fine-tuning.
"""

print(test_data)

"""## Step 4 - Apply Lora  
Here comes the magic with peft! Let's load a PeftModel and specify that we are going to use low-rank adapters (LoRA) using get_peft_model utility function and  the prepare_model_for_kbit_training method from PEFT.
"""

from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model
model.gradient_checkpointing_enable()
model = prepare_model_for_kbit_training(model)

print(model)

import bitsandbytes as bnb
def find_all_linear_names(model):
  cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)
  lora_module_names = set()
  for name, module in model.named_modules():
    if isinstance(module, cls):
      names = name.split('.')
      lora_module_names.add(names[0] if len(names) == 1 else names[-1])
    if 'lm_head' in lora_module_names: # needed for 16-bit
      lora_module_names.remove('lm_head')
  return list(lora_module_names)

modules = find_all_linear_names(model)
print(modules)

from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=64,
    lora_alpha=32,
    target_modules=modules,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)

trainable, total = model.get_nb_trainable_parameters()
print(f"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%")

"""## Step 5 - Run the training!

Setting the training arguments:
* for the reason of demo, we just ran it for few steps (100) just to showcase how to use this integration with existing tools on the HF ecosystem.
"""

# import transformers

# tokenizer.pad_token = tokenizer.eos_token


# trainer = transformers.Trainer(
#     model=model,
#     train_dataset=train_data,
#     eval_dataset=test_data,
#     args=transformers.TrainingArguments(
#         per_device_train_batch_size=1,
#         gradient_accumulation_steps=4,
#         warmup_steps=0.03,
#         max_steps=100,
#         learning_rate=2e-4,
#         fp16=True,
#         logging_steps=1,
#         output_dir="outputs_mistral_b_finance_finetuned_test",
#         optim="paged_adamw_8bit",
#         save_strategy="epoch",
#     ),
#     data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),
# )

"""### Fine-Tuning with qLora and Supervised Fine-Tuning

We're ready to fine-tune our model using qLora. For this tutorial, we'll use the `SFTTrainer` from the `trl` library for supervised fine-tuning. Ensure that you've installed the `trl` library as mentioned in the prerequisites.
"""

#new code using SFTTrainer
import transformers

from trl import SFTTrainer

tokenizer.pad_token = tokenizer.eos_token
torch.cuda.empty_cache()

trainer = SFTTrainer(
    model=model,
    train_dataset=train_data,
    eval_dataset=test_data,
    dataset_text_field="prompt",
    peft_config=lora_config,
    args=transformers.TrainingArguments(
        per_device_train_batch_size=1,
        gradient_accumulation_steps=4,
        warmup_steps=0.03,
        max_steps=100,
        learning_rate=2e-4,
        logging_steps=1,
        output_dir="outputs",
        optim="paged_adamw_8bit",
        save_strategy="epoch",
    ),
    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),
)

"""## Lets start training"""

model.config.use_cache = False  # silence the warnings. Please re-enable for inference!
trainer.train()

""" Share adapters on the ü§ó Hub"""

new_model = "gemma-Code-Instruct-Finetune-test" #Name of the model you will be pushing to huggingface model hub

trainer.model.save_pretrained(new_model)

base_model = AutoModelForCausalLM.from_pretrained(
    model_id,
    low_cpu_mem_usage=True,
    return_dict=True,
    torch_dtype=torch.float16,
    device_map={"": 0},
)
merged_model= PeftModel.from_pretrained(base_model, new_model)
merged_model= merged_model.merge_and_unload()

# Save the merged model
merged_model.save_pretrained("merged_model",safe_serialization=True)
tokenizer.save_pretrained("merged_model")
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

from google.colab import drive
import shutil

# Mount Google Drive
#drive.mount('/content/drive')

# Define the source and destination paths
source_path = '/content/merged_model'
destination_path = '/content/drive/MyDrive/merged_model'  # Adjust the destination folder if needed

# Copy the file (or directory)
try:
    shutil.copytree(source_path, destination_path) if os.path.isdir(source_path) else shutil.copy(source_path, destination_path)
    print(f"Successfully copied '{source_path}' to '{destination_path}'")
except FileNotFoundError:
    print(f"Error: '{source_path}' not found.")
except PermissionError:
    print(f"Error: Permission denied for '{destination_path}'. Make sure you have write access.")
except Exception as e:
    print(f"An unexpected error occurred: {e}")



"""## Test out Finetuned Model"""

result = get_completion(query="Hello explain the Certificationn", model=merged_model, tokenizer=tokenizer)
print(result)

